[
    {
        "explicit_name": null,
        "explicit_description": null,
        "forecast_bot_class_name": "FermiWithSearchControl",
        "num_input_questions": 1,
        "timestamp": "2025-05-13T16:56:29.936706",
        "time_taken_in_minutes": 0.9472436825434367,
        "total_cost": 0.0,
        "git_commit_hash": "102e14f",
        "forecast_bot_config": {
            "llm_model": "openrouter/openai/gpt-4o-mini",
            "llm_temperature": 0.2,
            "llms": {
                "default": {
                    "original_model": "gpt-4o",
                    "allowed_tries": 2,
                    "model": "gpt-4o",
                    "temperature": 0.3,
                    "timeout": 40
                },
                "summarizer": {
                    "original_model": "gpt-4o-mini",
                    "allowed_tries": 2,
                    "model": "gpt-4o-mini",
                    "temperature": 0.3,
                    "timeout": 40
                }
            }
        },
        "code": "class FermiWithSearchControl(ForecastBot):\n    def __init__(self, llm_model: str = \"gpt-4o-mini\", llm_temperature: float = 0.2):\n        super().__init__()\n        self.name = \"FermiWithSearchControl\"\n        self.llm_model = llm_model\n        self.llm_temperature = llm_temperature\n        self.llm = GeneralLlm(model=llm_model, temperature=llm_temperature)\n\n    async def run_research(self, question):\n        print(\"[FermiWithSearchControl] Entering run_research\")\n        try:\n            # Step 1: Fermi estimation and search term generation\n            fermi_prompt = clean_indents(\n                f\"\"\"\n                You are a professional forecaster skilled in Fermi estimation (back-of-the-envelope reasoning).\n                Your task is to answer the following question using a Fermi estimation approach.\n\n                {question.question_text}\n\n                Instructions:\n                - Break the problem down into smaller, logical components.\n                - For each component, make explicit, reasonable guesses or estimates, and clearly state your assumptions.\n                - Document every step and calculation in detail, showing your work.\n                - Do not make any unstated assumptions; explain your reasoning for each guess.\n                - Proceed step by step, combining your estimates to reach a final answer.\n                - At the end, summarize your Fermi estimate and show the final calculation.\n                - After your Fermi estimate, list between 1 and 4 web search queries (as an array of strings) that would help you reduce your uncertainty or check the weakest parts of your reasoning. If you don't need any, return an empty array [].\n                - Format your search queries as a valid Python list of strings, e.g. [\\\"search 1\\\", \\\"search 2\\\"].\n                \"\"\"\n            )\n            fermi_response = await self.llm.invoke(fermi_prompt)\n            print(\"[FermiWithSearchControl] Fermi response:\", fermi_response)\n\n            # Extract search queries (as a Python list) from the LLM output\n            import ast\n            import re\n            search_queries = []\n            match = re.search(r'\\[.*?\\]', fermi_response, re.DOTALL)\n            if match:\n                try:\n                    search_queries = ast.literal_eval(match.group(0))\n                    if not isinstance(search_queries, list):\n                        search_queries = []\n                except Exception as e:\n                    print(\n                        \"[FermiWithSearchControl] Error parsing search queries:\", e)\n                    traceback.print_exc()\n                    search_queries = []\n            print(\"[FermiWithSearchControl] Search queries:\", search_queries)\n\n            # Step 2: Run OpenRouter web search for each query\n            search_results = []\n            for query_str in search_queries:\n                try:\n                    result = get_web_search_results_from_openrouter(query_str)\n                except Exception as e:\n                    print(\n                        f\"[FermiWithSearchControl] Error during web search for query '{query_str}':\", e)\n                    traceback.print_exc()\n                    result = f\"Error: {e}\"\n                print(\n                    f\"[FermiWithSearchControl] Search result for '{query_str}':\", result)\n                search_results.append({\"query\": query_str, \"result\": result})\n\n            # Step 3: Aggregate all research\n            research = {\n                \"fermi_response\": fermi_response,\n                \"search_queries\": search_queries,\n                \"search_results\": search_results\n            }\n            print(\n                \"[FermiWithSearchControl] Exiting run_research with research:\", research)\n            return research\n        except Exception as e:\n            print(\"[FermiWithSearchControl] Exception in run_research:\", e)\n            traceback.print_exc()\n            return {\"fermi_response\": \"\", \"search_queries\": [], \"search_results\": [], \"error\": str(e)}\n\n    async def _run_forecast_on_binary(self, question: BinaryQuestion, research) -> ReasonedPrediction[float]:\n        print(\"[FermiWithSearchControl] Entering _run_forecast_on_binary\")\n        try:\n            fermi_response = research[\"fermi_response\"] if isinstance(\n                research, dict) else \"\"\n            search_queries = research[\"search_queries\"] if isinstance(\n                research, dict) else []\n            search_results = research[\"search_results\"] if isinstance(\n                research, dict) else []\n            search_results_str = \"\\n\\n\".join([\n                f\"Search: {item['query']}\\nResult: {item['result']}\" for item in search_results\n            ])\n            prompt = clean_indents(\n                f\"\"\"\n                You are a professional forecaster skilled in Fermi estimation (back-of-the-envelope reasoning).\n                Here is your original Fermi estimate and reasoning:\n                {fermi_response}\n\n                You identified these search queries to reduce your uncertainty:\n                {search_queries}\n\n                Here are the web search results for each query:\n                {search_results_str}\n\n                Please review your original Fermi estimate in light of this new information. Revise your reasoning and final answer if needed. Clearly state any changes or updates.\n\n                The last thing you write is your final answer as: \"Probability: ZZ%\", 0-100\n                \"\"\"\n            )\n            reasoning = await self.llm.invoke(prompt)\n            prediction = PredictionExtractor.extract_last_percentage_value(\n                reasoning, max_prediction=1, min_prediction=0)\n            print(\n                \"[FermiWithSearchControl] Exiting _run_forecast_on_binary with prediction:\", prediction)\n            return ReasonedPrediction(prediction_value=prediction, reasoning=reasoning)\n        except Exception as e:\n            print(\"[FermiWithSearchControl] Exception in _run_forecast_on_binary:\", e)\n            traceback.print_exc()\n            return ReasonedPrediction(prediction_value=0.0, reasoning=f\"Exception: {e}\")\n\n    async def _run_forecast_on_multiple_choice(self, question: MultipleChoiceQuestion, research) -> ReasonedPrediction[PredictedOptionList]:\n        print(\"[FermiWithSearchControl] Entering _run_forecast_on_multiple_choice\")\n        try:\n            fermi_response = research[\"fermi_response\"] if isinstance(\n                research, dict) else \"\"\n            search_queries = research[\"search_queries\"] if isinstance(\n                research, dict) else []\n            search_results = research[\"search_results\"] if isinstance(\n                research, dict) else []\n            search_results_str = \"\\n\\n\".join([\n                f\"Search: {item['query']}\\nResult: {item['result']}\" for item in search_results\n            ])\n            prompt = clean_indents(\n                f\"\"\"\n                You are a professional forecaster skilled in Fermi estimation (back-of-the-envelope reasoning).\n                Here is your original Fermi estimate and reasoning:\n                {fermi_response}\n\n                You identified these search queries to reduce your uncertainty:\n                {search_queries}\n\n                Here are the web search results for each query:\n                {search_results_str}\n\n                Please review your original Fermi estimate in light of this new information. Revise your reasoning and final answer if needed. Clearly state any changes or updates.\n\n                The last thing you write is your final probabilities for the N options in this order {question.options} as:\n                Option_A: Probability_A\n                Option_B: Probability_B\n                ...\n                Option_N: Probability_N\n                \"\"\"\n            )\n            reasoning = await self.llm.invoke(prompt)\n            prediction = PredictionExtractor.extract_option_list_with_percentage_afterwards(\n                reasoning, question.options)\n            print(\n                \"[FermiWithSearchControl] Exiting _run_forecast_on_multiple_choice with prediction:\", prediction)\n            return ReasonedPrediction(prediction_value=prediction, reasoning=reasoning)\n        except Exception as e:\n            print(\n                \"[FermiWithSearchControl] Exception in _run_forecast_on_multiple_choice:\", e)\n            traceback.print_exc()\n            return ReasonedPrediction(prediction_value=[], reasoning=f\"Exception: {e}\")\n\n    async def _run_forecast_on_numeric(self, question: NumericQuestion, research) -> ReasonedPrediction[NumericDistribution]:\n        print(\"[FermiWithSearchControl] Entering _run_forecast_on_numeric\")\n        try:\n            fermi_response = research[\"fermi_response\"] if isinstance(\n                research, dict) else \"\"\n            search_queries = research[\"search_queries\"] if isinstance(\n                research, dict) else []\n            search_results = research[\"search_results\"] if isinstance(\n                research, dict) else []\n            search_results_str = \"\\n\\n\".join([\n                f\"Search: {item['query']}\\nResult: {item['result']}\" for item in search_results\n            ])\n            lower = getattr(question, 'lower_bound', 0)\n            upper = getattr(question, 'upper_bound', 100)\n            lower_bound_message = f\"The outcome can not be lower than {lower}.\" if hasattr(\n                question, 'lower_bound') else \"\"\n            upper_bound_message = f\"The outcome can not be higher than {upper}.\" if hasattr(\n                question, 'upper_bound') else \"\"\n            prompt = clean_indents(\n                f\"\"\"\n                You are a professional forecaster skilled in Fermi estimation (back-of-the-envelope reasoning).\n                Here is your original Fermi estimate and reasoning:\n                {fermi_response}\n\n                You identified these search queries to reduce your uncertainty:\n                {search_queries}\n\n                Here are the web search results for each query:\n                {search_results_str}\n\n                {lower_bound_message}\n                {upper_bound_message}\n\n                Please review your original Fermi estimate in light of this new information. Revise your reasoning and final answer if needed. Clearly state any changes or updates.\n\n                The last thing you write is your final answer as:\n                \"\n                Percentile 10: XX\n                Percentile 20: XX\n                Percentile 40: XX\n                Percentile 60: XX\n                Percentile 80: XX\n                Percentile 90: XX\n                \"\n                \"\"\"\n            )\n            reasoning = await self.llm.invoke(prompt)\n            prediction = PredictionExtractor.extract_numeric_distribution_from_list_of_percentile_number_and_probability(\n                reasoning, question)\n            print(\n                \"[FermiWithSearchControl] Exiting _run_forecast_on_numeric with prediction:\", prediction)\n            return ReasonedPrediction(prediction_value=prediction, reasoning=reasoning)\n        except Exception as e:\n            print(\"[FermiWithSearchControl] Exception in _run_forecast_on_numeric:\", e)\n            traceback.print_exc()\n            return ReasonedPrediction(prediction_value=None, reasoning=f\"Exception: {e}\")\n",
        "forecast_reports": []
    }
]